This document explains the structure and workflow of the `app.py` script, a Streamlit-based chatbot named "FloatChat" designed to interact with ARGO oceanographic data.

### High-Level Overview

FloatChat provides a conversational interface (a chatbot) for users to query and visualize ARGO float data. Users can ask questions in natural language (e.g., "show me the temperature and pressure data in the Bay of Bengal" or "map the dissolved oxygen data quality"). The application interprets these questions, fetches the relevant data using the `argopy` library, generates plots or maps if requested, and presents the findings to the user.

The application is built on three main pillars:
1.  **Streamlit:** For creating the web-based user interface (the chat window).
2.  **LangChain:** For creating an "agent" that can reason, use tools, and respond to user queries.
3.  **Argopy:** A Python library for fetching and working with ARGO data.

### Data and Control Flow: From Prompt to Response

Here is a step-by-step breakdown of how the application processes a user's request:

1.  **User Input:** The user types a message into the chat input field in the Streamlit web interface and presses Enter.

2.  **Streamlit Captures Input:** The `st.chat_input` function in the "Streamlit frontend" section of the code captures the user's text.

3.  **Agent Invocation:** The captured text (the user's prompt) is passed to the `agent_executor.invoke()` function. This is the entry point into the LangChain agent system.

4.  **Agent Reasoning (The "Thought" Process):
    *   The agent, powered by the `gemini-2.0-flash` model and a specialized prompt (`react_prompt_template`), analyzes the user's input.
    *   It decides which of the available tools (`fetch_argo_data_by_region_plot` or `generate_bgc_parameter_map`) is best suited to answer the question.
    *   It formulates a plan, which is visible in the terminal/logs as the "Thought" process.

5.  **Action and Action Input:**
    *   The agent selects a tool to use (the "Action").
    *   Crucially, it constructs a JSON object containing the parameters for that tool (the "Action Input"). For example, for the prompt "show me temperature vs pressure near lat 15, lon 88", it might generate the Action `fetch_argo_data_by_region_plot` with the Action Input `{"lat": 15, "lon": 88, "plot": true, "x": "PRES", "y": "TEMP"}`.

6.  **Tool Execution:**
    *   The `AgentExecutor` calls the chosen Python function (the tool) and passes the generated JSON object as its argument.
    *   Before the tool's main logic runs, helper functions like `parse_action_input` or `parse_bgc_tool_input` clean up the input string, removing markdown code blocks and parsing it into a Python dictionary.
    *   The tool then executes its logic:
        *   It connects to the ARGO data servers using `argopy`.
        *   It fetches the data based on the parameters (region, time, platform, etc.).
        *   If plotting is requested (`"plot": true`), it uses `plotly.express` to create a chart or map.
        *   It saves the generated image to the `out_img/` directory with a unique filename.
        *   The tool returns a string result (the "Observation"), which includes a success message and the path to the saved image.

7.  **Observation and Final Answer:**
    *   The agent receives the string returned by the tool (the "Observation").
    *   Based on the prompt instructions, if the observation indicates success ("âœ… SUCCESS"), the agent knows its job is done.
    *   It then formulates a "Final Answer" for the user. This is a user-friendly summary of the findings, incorporating information from the observation (e.g., "I have fetched the data for the specified region and generated a plot. The plot is saved at `out_img/plot_...png`").

8.  **Displaying the Response in Streamlit:**
    *   The `response_text` (the agent's "Final Answer") is displayed in the chat window using `st.markdown()`.
    *   The code then uses regular expressions (`re.search`) to find the image path mentioned in the response text.
    *   If an image path is found and the file exists, `st.image()` is used to display the generated plot directly in the chat window.
    *   Finally, the user's message and the assistant's full response (text and image) are saved to the `st.session_state.messages` list to maintain the chat history.

### Function Breakdown

#### Helper Functions
*   `parse_action_input(tool_input)` and `parse_bgc_tool_input(tool_input)`: These are essential utility functions that act as a bridge between the LangChain agent and the Python tools. The agent often wraps its `Action Input` in markdown (e.g., ```json
{...}
```). These functions reliably strip away that wrapping and parse the clean JSON string into a Python dictionary that the tools can work with.

#### Tools
*   `@tool()`: This decorator from LangChain registers the function as a tool that the agent can decide to call.

*   `fetch_argo_data_by_region_plot(tool_input)`: This is the primary tool for fetching core ARGO data (like temperature, salinity, pressure).
    *   **Flexibility:** It's designed to be highly flexible, accepting single coordinates, bounding boxes, or various data filters (platform number, time range, etc.).
    *   **Data Fetching:** It uses `argopy.DataFetcher` to get the data and converts it into a Pandas DataFrame.
    *   **Filtering:** It applies any additional filters provided (e.g., temperature range).
    *   **Plotting:** If requested, it uses `plotly.express` to create scatter or line plots and saves them as PNG files in the `out_img` directory.
    *   **Output:** Returns a string summarizing the result and providing the path to the output image if one was created.

*   `generate_bgc_parameter_map(tool_input)`: This tool is specialized for Bio-Geo-Chemical (BGC) data.
    *   **Functionality:** It creates a global scatter map showing the location and data quality mode (`R`, `A`, or `D`) for a specific BGC parameter (like `DOXY` for oxygen or `CHLA` for chlorophyll).
    *   **Data Indexing:** It uses `argopy.ArgoIndex` to find all float profiles that contain the requested parameter.
    *   **Mapping:** It uses `plotly.express.scatter_geo` to plot the data on a world map.
    *   **Output:** Saves the map as a PNG file in `out_img` and returns a success message with the file path.

#### LLM and Agent Setup
*   `ChatGoogleGenerativeAI`: Initializes the connection to the Google Gemini LLM.
*   `react_prompt_template`: This is a crucial and detailed prompt that instructs the agent on how to behave. It defines the "ReAct" (Reasoning and Acting) framework. It tells the agent:
    *   What tools it has access to.
    *   **Crucially, how to format the `Action Input` as a valid JSON object.**
    *   The expected thought process (Thought -> Action -> Action Input -> Observation).
    *   When to stop and provide a final answer.
*   `create_react_agent`: Combines the LLM, the tools, and the prompt template into a functioning agent.
*   `AgentExecutor`: This is the runtime environment for the agent. It takes the agent's decisions (actions) and executes them, feeding the results back to the agent until a final answer is reached.

#### Streamlit Frontend
*   `st.title`, `st.chat_message`, `st.chat_input`: Standard Streamlit functions to build the chat interface.
*   `st.session_state.messages`: A list that stores the history of the conversation, allowing the chat to be stateful.
*   `st.spinner("Thinking...")`: Provides visual feedback to the user while the agent is processing the request.
*   `re.search` and `st.image`: The logic to find and display the generated images within the chat response, creating a rich, multimedia experience for the user.
